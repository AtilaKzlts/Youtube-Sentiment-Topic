{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9znYMWaMO1a"
   },
   "source": [
    "# **Youtube  Comment Sentiment & Topic Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_vpVJOwWYCg"
   },
   "source": [
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uI-aUKYH1yyL",
    "outputId": "482b00ee-b51b-4331-8513-a8b38b33c46f"
   },
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# YouTube API ayarlarƒ±\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "DEVELOPER_KEY = \"AIzaSyDjud_z8QE1DuZ45456jbwJlzz5PJApAw0\"\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=DEVELOPER_KEY\n",
    ")\n",
    "\n",
    "# Yorum √ßekme fonksiyonu\n",
    "def fetch_comments(video_id, comment_limit=5000):\n",
    "    comments = []\n",
    "    fetched_comment_count = 0\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        if fetched_comment_count >= comment_limit:\n",
    "            break\n",
    "\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            maxResults=100,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            if fetched_comment_count >= comment_limit:\n",
    "                break\n",
    "\n",
    "            top_comment = item['snippet']['topLevelComment']\n",
    "            comment_snippet = top_comment['snippet']\n",
    "            comment_id = top_comment['id']\n",
    "\n",
    "            comments.append({\n",
    "                \"video_id\": video_id,\n",
    "                \"comment_id\": comment_id,\n",
    "                \"parent_id\": None,\n",
    "                \"is_reply\": False,\n",
    "                \"published_at\": comment_snippet['publishedAt'],\n",
    "                \"like_count\": comment_snippet['likeCount'],\n",
    "                \"text\": comment_snippet['textDisplay']\n",
    "            })\n",
    "            fetched_comment_count += 1\n",
    "\n",
    "            # Yanƒ±tlarƒ± da √ßek\n",
    "            if item['snippet']['totalReplyCount'] > 0:\n",
    "                replies_request = youtube.comments().list(\n",
    "                    part=\"snippet\",\n",
    "                    parentId=comment_id,\n",
    "                    maxResults=100\n",
    "                )\n",
    "                replies_response = replies_request.execute()\n",
    "\n",
    "                for reply in replies_response.get(\"items\", []):\n",
    "                    if fetched_comment_count >= comment_limit:\n",
    "                        break\n",
    "\n",
    "                    reply_snippet = reply['snippet']\n",
    "                    comments.append({\n",
    "                        \"video_id\": video_id,\n",
    "                        \"comment_id\": reply['id'],\n",
    "                        \"parent_id\": comment_id,\n",
    "                        \"is_reply\": True,\n",
    "                        \"published_at\": reply_snippet['publishedAt'],\n",
    "                        \"like_count\": reply_snippet['likeCount'],\n",
    "                        \"text\": reply_snippet['textDisplay']\n",
    "                    })\n",
    "                    fetched_comment_count += 1\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "# TOGG video ID'leri\n",
    "video_ids = [\n",
    "    \"y7hz_WEvIyA\",      # TOGG tanƒ±tƒ±m videosu\n",
    "    \"EwofPjL0bqg\",      # TOGG 2. video\n",
    "    \"FukC6_jVD24\"       # TOGG 3. video\n",
    "]\n",
    "\n",
    "# T√ºm yorumlarƒ± topla\n",
    "all_comments = []\n",
    "for vid in video_ids:\n",
    "    print(f\"Yorumlar √ßekiliyor: {vid}\")\n",
    "    try:\n",
    "        comments = fetch_comments(vid, comment_limit=5000)\n",
    "        all_comments.extend(comments)\n",
    "        time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Hata olu≈ütu: {vid} - {e}\")\n",
    "\n",
    "# DataFrame olu≈ütur\n",
    "df = pd.DataFrame(all_comments)\n",
    "print(\"Toplam yorum sayƒ±sƒ±:\", df.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEKUZEi3YVSl"
   },
   "source": [
    " *Let`s see what we pulled!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TvlZX511wDZd",
    "outputId": "0d41fd5b-f7f2-4897-f957-3104ab01e0dc"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1XU6ZM4cPDx"
   },
   "source": [
    "### **Text Leng Check**\n",
    "+ Outlier Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPGZjTsqaLZU",
    "outputId": "7fb47e33-1853-4d0b-8eae-1611ff667fb7"
   },
   "outputs": [],
   "source": [
    "# df['text'].str.len().max() # En uzun kac karakter\n",
    "\n",
    "df['text'].str.len().idxmax() # En uzunun id`si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGxspwCBhCHh"
   },
   "outputs": [],
   "source": [
    "#df.drop(3047, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "fHCXPFkLhblB",
    "outputId": "21ceed6b-0bbd-4203-b30f-eff8b78fc50b"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['text'].str.len().value_counts().plot(\n",
    "    kind='hist',\n",
    "    bins=15,\n",
    "    title='Text Lenght'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mQE3IUACtwX"
   },
   "source": [
    "### **Response status distribution and Number of comments per hour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_oi419NcJdg"
   },
   "outputs": [],
   "source": [
    "df['published_at'] = pd.to_datetime(df['published_at'])\n",
    "\n",
    "df['year']  = df['published_at'].dt.year\n",
    "df['month']  = df['published_at'].dt.month\n",
    "df['hour']  = df['published_at'].dt.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osx_o4V6ZpDi"
   },
   "outputs": [],
   "source": [
    "saatline = df.groupby('hour').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "8I_vAhUZBoW8",
    "outputId": "8a4d247b-db66-4580-c2b8-eac4d7cc60fe"
   },
   "outputs": [],
   "source": [
    "# Create subplots: one for reply status, one for hourly comment counts\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Count of comments that are replies vs not\n",
    "sns.countplot(y='is_reply', data=df, hue='is_reply', ax=axes[0])\n",
    "axes[0].set_title('Reply Status Counts')\n",
    "axes[0].set_xlabel('Count')\n",
    "axes[0].set_ylabel('Reply Status')\n",
    "\n",
    "# Plot 2: Line plot showing comment count per hour\n",
    "saatline.plot(kind='line', marker='o', ax=axes[1])\n",
    "axes[1].set_title('Hourly Comment Counts')\n",
    "axes[1].set_xlabel('Hour')\n",
    "axes[1].set_ylabel('Number of Comments')\n",
    "axes[1].set_xticks(range(24))\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFNWRC6FibcC"
   },
   "source": [
    "## **Text Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSXFK4ldCFCa"
   },
   "source": [
    " What You Shouldn't Do?\n",
    "Stopword removal, stemming, lemmatization ‚Üí UNNECESSARY!\n",
    "Because **BERT** already understands the context. This is classic NLP, it doesn't apply here.\n",
    "+ And since bert understands emojis, we don't need to delete them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RGyp36V59-2q",
    "outputId": "04d0a199-3c7c-4d1d-f025-c87d54bcca37"
   },
   "outputs": [],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O0VwkShb-z_q",
    "outputId": "3050449f-9874-4c00-bb07-071621283809"
   },
   "outputs": [],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnp4-s4iidwE"
   },
   "outputs": [],
   "source": [
    "# Gerekli k√ºt√ºphaneleri i√ße aktar\n",
    "import re\n",
    "import string\n",
    "import html\n",
    "import contractions\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# 1. K√º√ß√ºk harfe √ßevirme\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "# 2. Contraction a√ßma (√∂rn: I'm -> I am)\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "\n",
    "# 4. URL ve Mention (@username) kaldƒ±rma\n",
    "def remove_urls_mentions(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # URL'leri kaldƒ±r\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)      # Mention'larƒ± kaldƒ±r\n",
    "    return text\n",
    "\n",
    "\n",
    "# 5. HTML etiketleri temizleme (√∂rn: <br> gibi)\n",
    "def remove_html(text):\n",
    "    return html.unescape(re.sub(r'<.*?>', '', text))\n",
    "\n",
    "\n",
    "# 6. Spam kelimeleri temizleme (√∂rn: \"buy now\", \"click here\")\n",
    "def remove_spam_words(text):\n",
    "    spam_words = ['buy now', 'click here', 'subscribe', 'free offer']\n",
    "    for word in spam_words:\n",
    "        text = text.replace(word, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "# 7. Noktalama i≈üaretlerini kaldƒ±rma\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "\n",
    "# 8. Harf uzatmalarƒ±nƒ± normalize etme (√∂rn: \"soooo\" -> \"so\")\n",
    "def normalize_elongations(text):\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "\n",
    "# 10. Fazla bo≈üluklarƒ± temizleme\n",
    "def final_cleanup(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "# Temizlik adƒ±mlarƒ±nƒ± y√∂neten ana sƒ±nƒ±f\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "\n",
    "        pass\n",
    "\n",
    "    def clean(self, text):\n",
    "        text = to_lowercase(text)\n",
    "        text = expand_contractions(text)\n",
    "        text = remove_urls_mentions(text)\n",
    "        text = remove_html(text)\n",
    "        text = remove_spam_words(text)\n",
    "        text = remove_punctuation(text)\n",
    "        text = normalize_elongations(text)\n",
    "        text = final_cleanup(text)\n",
    "\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd2bdC3-_1ka"
   },
   "outputs": [],
   "source": [
    "cleaner = TextCleaner()\n",
    "\n",
    "# 'text' kolonunda temizlik yap ve yeni bir kolon olu≈ütur\n",
    "df['clean_text'] = df['text'].apply(lambda x: cleaner.clean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzLqPp1ikSK5"
   },
   "source": [
    "## Language detection;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ZrZQ0Gmk7pB",
    "outputId": "35e64226-f9a3-4673-f8f4-98c8fe231cd1"
   },
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyDTNtSfkV6J"
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "def guvenli_dil_tespiti(text):\n",
    "    if isinstance(text, str) and text.strip():  # Hem string kontrol√º hem de bo≈üluk kontrol√º\n",
    "        try:\n",
    "            return detect(text)\n",
    "        except LangDetectException as e:\n",
    "            if e.args[0] == 'No features in text.':\n",
    "                return 'unknown'\n",
    "            else:\n",
    "                raise  # Diƒüer LangDetectException hatalarƒ±nƒ± yeniden y√ºkselt\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "\n",
    "# Yeni kolon: dil bilgisi\n",
    "df['lang'] = df['text'].apply(guvenli_dil_tespiti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80DoO0IKsA2J"
   },
   "outputs": [],
   "source": [
    "df_eng = df[df['lang'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pq-Ed5CivVOR",
    "outputId": "424b4242-c118-4b81-f7cc-fba9151a3c15"
   },
   "outputs": [],
   "source": [
    "df_eng.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyEXZRvB8SyB"
   },
   "source": [
    "## **Repeated line drop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQ-4W3POxvMB",
    "outputId": "777e56f1-ba88-42c7-9683-51783089a658"
   },
   "outputs": [],
   "source": [
    "# Check the number of duplicate rows with the same 'text' content\n",
    "duplicate_rows = df_eng[df_eng.duplicated(subset='text', keep=False)]\n",
    "print(f\"Number of duplicate rows with identical comments: {duplicate_rows.shape[0]}\")\n",
    "\n",
    "# Remove duplicate rows based on 'text', keeping the first occurrence, and reset the index\n",
    "df_eng = df_eng.drop_duplicates(subset='text', keep='first').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7ovMt15ml5x"
   },
   "source": [
    "## **Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6CXhocvArDY"
   },
   "source": [
    "### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bAaoh3tnAuCP",
    "outputId": "f24415f2-37d2-4279-9eb1-5a9bdccb1705"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJHC4XFGFcDZ",
    "outputId": "261a0a78-6eb8-41cc-ca09-0143389852ca"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    tokenizer=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "\n",
    "def get_sentiment(text):\n",
    "    result = sentiment_pipeline(text)\n",
    "    return result[0]['label']\n",
    "\n",
    "# Apply\n",
    "df_eng['sentiment'] = df_eng['clean_text'].apply(get_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBiLn6bXhNcV"
   },
   "source": [
    "### **Model Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "xAztSSrPTHlh",
    "outputId": "eed82ee4-facc-462c-a95f-c52b5a603429"
   },
   "outputs": [],
   "source": [
    "print(df_eng['sentiment'].value_counts())\n",
    "\n",
    "df_eng['sentiment'].value_counts().plot(kind='bar', color=['green', 'red'])\n",
    "plt.title('Fine-tuned')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "rkE64-Elmwqy",
    "outputId": "5decf35a-bdc7-4ab8-8e43-760a4ce488ee"
   },
   "outputs": [],
   "source": [
    "# Group comments by date and sentiment, then count occurrences\n",
    "daily_sentiment = df_eng.groupby([df_eng['published_at'].dt.date, 'sentiment']).size().unstack().fillna(0)\n",
    "\n",
    "# Plot sentiment change over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=daily_sentiment)\n",
    "plt.title(\"Sentiment Trends Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of Comments\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "wrQKCWNlm4lm",
    "outputId": "aea444a2-e6dd-476d-f5fb-2a921b7f8ba1"
   },
   "outputs": [],
   "source": [
    "# Group by hour and sentiment, then count the number of comments\n",
    "hourly_sentiment = df_eng.groupby(['hour', 'sentiment']).size().unstack().fillna(0)\n",
    "\n",
    "# Heatmap visualization\n",
    "plt.figure(figsize=(13, 6))\n",
    "sns.heatmap(hourly_sentiment.T, cmap='coolwarm', annot=True, fmt=\".0f\")\n",
    "plt.title(\"Hourly Sentiment Intensity\")\n",
    "plt.xlabel(\"Hour of Day\")\n",
    "plt.ylabel(\"Sentiment\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdDvMuF-o1id"
   },
   "source": [
    "## **Most Repeated Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bam7-aMcvPUW"
   },
   "outputs": [],
   "source": [
    "emoji_words = [\n",
    "    \"face_with_tears_of_joy\", \"red_heart\", \"clapping_hands\", \"blue_heart\",\n",
    "    \"smiling_face\", \"fire\", \"folded_hands\", \"crying_face\", \"thumbs_up\",\n",
    "    \"heart_eyes\", \"grinning_face\", \"thinking_face\", \"expressionless_face\",\n",
    "    \"see_no_evil_monkey\", \"raising_hands\", \"eyes\", \"rocket\", \"hundred_points\",\n",
    "    \"redheart\",\"facewithtearsofjoy\",'clappinghandslightskintone'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3gwhu4spvYp"
   },
   "outputs": [],
   "source": [
    "positive_texts = df_eng[df_eng[\"sentiment\"] == \"POSITIVE\"][\"clean_text\"]\n",
    "negative_texts = df_eng[df_eng[\"sentiment\"] == \"NEGATIVE\"][\"clean_text\"]\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_filtered_word_counts(text_series):\n",
    "    words = []\n",
    "    for text in text_series:\n",
    "        for word in text.split():\n",
    "            if word.lower() not in ENGLISH_STOP_WORDS.union(set(emoji_words)):\n",
    "                words.append(word.lower())\n",
    "    return Counter(words)\n",
    "\n",
    "\n",
    "positive_word_counts = get_filtered_word_counts(positive_texts)\n",
    "negative_word_counts = get_filtered_word_counts(negative_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljR0njcsq4lU",
    "outputId": "63d4c6e2-2efd-4001-e960-6b498d00633b"
   },
   "outputs": [],
   "source": [
    "print(\"Total number of positive comments:\", len(positive_texts))\n",
    "print(\"Total number of negative comments:\", len(negative_texts))\n",
    "\n",
    "# Positive word pool\n",
    "all_pos_words = \" \".join(positive_texts).split()\n",
    "print(f\"Total number of positive words: {len(all_pos_words)}\")\n",
    "print(\"First 20 words:\", all_pos_words[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CqAzEDiSncG1",
    "outputId": "d4625560-52cc-4454-e350-4fce2d5691cd"
   },
   "outputs": [],
   "source": [
    "print(\"üîµ Most frequent POSITIVE words:\")\n",
    "print(positive_word_counts.most_common(10))\n",
    "\n",
    "print(\"\\nüî¥ Most frequent NEGATIVE words:\")\n",
    "print(negative_word_counts.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0NXA0jtwkA9"
   },
   "source": [
    "## **N Gram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rfuWqnL0UbN"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QNyOWLmvwlFV",
    "outputId": "bdba144a-a1a6-43e6-efeb-52c0fd48d617"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_top_ngrams(text_series, n=2, stopwords=None):\n",
    "    \"\"\"\n",
    "    N-gram'larƒ± √ºretmek ve en sƒ±k ge√ßenleri bulmak i√ßin fonksiyon.\n",
    "    n=2, bigram; n=3, trigram analizi yapar.\n",
    "    \"\"\"\n",
    "    ngram_list = []\n",
    "    for text in text_series.dropna():\n",
    "        words = [w.lower() for w in text.split() if w.lower() not in stopwords]\n",
    "\n",
    "        if n == 2:\n",
    "            ngram_list.extend(list(bigrams(words)))\n",
    "        elif n == 3:\n",
    "            ngram_list.extend(list(trigrams(words)))\n",
    "\n",
    "    return Counter(ngram_list)\n",
    "\n",
    "def plot_top_ngrams(ngram_counter, title, n=10):\n",
    "    \"\"\"\n",
    "    En sƒ±k ge√ßen N-gram'larƒ± g√∂rselle≈ütirmek i√ßin fonksiyon\n",
    "    \"\"\"\n",
    "    top_ngrams = ngram_counter.most_common(n)\n",
    "    ngram_phrases = [' '.join(ngram) for ngram, count in top_ngrams]\n",
    "    counts = [count for ngram, count in top_ngrams]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=counts, y=ngram_phrases, palette='viridis')\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"N-grams\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Pozitif ve negatif yorumlarƒ± kullanarak n-gramlarƒ± hesaplayalƒ±m\n",
    "positive_ngrams = get_top_ngrams(positive_texts, n=2, stopwords=ENGLISH_STOP_WORDS.union(set(emoji_words)))\n",
    "negative_ngrams = get_top_ngrams(negative_texts, n=2, stopwords=ENGLISH_STOP_WORDS.union(set(emoji_words)))\n",
    "\n",
    "# En sƒ±k ge√ßen 10 bigram'ƒ± g√∂rselle≈ütirelim\n",
    "plot_top_ngrams(positive_ngrams, \" Most Frequent Positive Bigrams\", n=10)\n",
    "plot_top_ngrams(negative_ngrams, \" Most Frequent Negative Bigrams\", n=10)\n",
    "\n",
    "# Trigram analizi (3 kelimelik kombinasyonlar)\n",
    "positive_trigrams = get_top_ngrams(positive_texts, n=3, stopwords=ENGLISH_STOP_WORDS.union(set(emoji_words)))\n",
    "negative_trigrams = get_top_ngrams(negative_texts, n=3, stopwords=ENGLISH_STOP_WORDS.union(set(emoji_words)))\n",
    "\n",
    "# En sƒ±k ge√ßen 10 trigram'ƒ± g√∂rselle≈ütirelim\n",
    "plot_top_ngrams(positive_trigrams, \"Most Frequent Positive Trigrams\", n=10)\n",
    "plot_top_ngrams(negative_trigrams, \"Most Frequent Negative Trigrams\", n=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RI57H1sYKJcL"
   },
   "source": [
    "## **Bert Topic**\n",
    "+ We couldn't remove emojis for sentiment analysis because the model we use can understand emojis, but we need to remove them for topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwbKQRf28r7a"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "df_eng[\"clean_no_emoji\"] = df_eng[\"clean_text\"].apply(remove_emojis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYMAx9Da09ef",
    "outputId": "1a845bdf-4c34-431e-f4a6-a63e2d028000"
   },
   "outputs": [],
   "source": [
    "!pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 827,
     "referenced_widgets": [
      "07127d9df12c49859bd405669d173959",
      "ca71567bf67f40a89f2e3e3502c47225",
      "b110ff373a9f4ef7b53c92b46234c587",
      "cbb7369fbd2b40178e4429044c45bb6f",
      "6fccd1f6dd784bd79221b4d20db74924",
      "e206a6ff2113435ebcb7c18f1b8050f3",
      "d02daa13047e43bbb99934d14af4ea44",
      "30734bce91574140ab2e6545f1321012",
      "ae1088fda5294b888973072cb2d283a7",
      "31eb8b3700b844e59fc08187f6db81e8",
      "5f1b085f2f9d4c3bba056392bbfad65a"
     ]
    },
    "id": "kVClX2cHWwyT",
    "outputId": "5a33833a-cd81-40ce-ee27-34f0031ab44d"
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Temizlik\n",
    "df_cleaned = df_eng[df_eng[\"clean_no_emoji\"].str.strip().notnull()]\n",
    "comments = df_cleaned[\"clean_no_emoji\"].tolist()\n",
    "\n",
    "print(f\"Toplam yorum sayƒ±sƒ±: {len(comments)}\")\n",
    "\n",
    "# 2. √áOK DAHA AZ STOPWORD (sadece ger√ßekten gereksizler)\n",
    "minimal_stopwords = [\n",
    "    \"video\", \"channel\", \"subscribe\", \"like\", \"watch\",\n",
    "    \"good\", \"great\", \"nice\", \"love\", \"thanks\", \"thank\"\n",
    "    # Diƒüerlerini √ßƒ±kardƒ±m - anlamlƒ± olabilirler\n",
    "]\n",
    "\n",
    "# 3. √á√ñZ√úM 1: Veri boyutunuzu s√∂yleyin, ona g√∂re ayarlayalƒ±m\n",
    "data_size = len(comments)\n",
    "print(f\"Veri boyutu: {data_size}\")\n",
    "\n",
    "# √áOK DAHA K√ú√á√úK CLUSTER BOYUTLARI\n",
    "if data_size < 1000:\n",
    "    min_cluster_size = 5\n",
    "    min_samples = 2\n",
    "elif data_size < 5000:\n",
    "    min_cluster_size = 8\n",
    "    min_samples = 3\n",
    "else:\n",
    "    min_cluster_size = 15\n",
    "    min_samples = 5\n",
    "\n",
    "print(f\"Yeni min_cluster_size: {min_cluster_size}\")\n",
    "print(f\"Yeni min_samples: {min_samples}\")\n",
    "\n",
    "# 4. DAHA AZ AGRESIF UMAP\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=10,        # Daha k√º√ß√ºk (15‚Üí10)\n",
    "    n_components=10,       # Daha fazla boyut (5‚Üí10)\n",
    "    min_dist=0.1,         # Biraz gev≈üek (0.0‚Üí0.1)\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 5. DAHA AZ AGRESIF VECTORIZER\n",
    "vectorizer_model = CountVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=minimal_stopwords,  # Az stopword\n",
    "    max_features=2000,             # Daha fazla feature (1000‚Üí2000)\n",
    "    min_df=1,                      # Daha az filtreleme (2‚Üí1)\n",
    "    max_df=0.90                    # Daha az filtreleme (0.8‚Üí0.95)\n",
    ")\n",
    "\n",
    "# 6. HDBSCAN\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=min_cluster_size,\n",
    "    min_samples=min_samples,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom',\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "# 7. BERTopic\n",
    "topic_model = BERTopic(\n",
    "    language=\"english\",\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 8. Eƒüitim\n",
    "print(\"\\nüöÄ Model eƒüitimi ba≈ülƒ±yor...\")\n",
    "topics, probs = topic_model.fit_transform(comments)\n",
    "\n",
    "# 9. Sonu√ßlar\n",
    "n_topics = len(set(topics))\n",
    "n_outliers = sum(1 for t in topics if t == -1)\n",
    "outlier_ratio = n_outliers / len(topics) * 100\n",
    "\n",
    "print(f\"\\nüìä SONU√áLAR:\")\n",
    "print(f\"Topic sayƒ±sƒ±: {n_topics}\")\n",
    "print(f\"Outlier oranƒ±: {outlier_ratio:.1f}%\")\n",
    "\n",
    "# 10. Hala az topic varsa, daha da agresif ol\n",
    "if n_topics < 8:\n",
    "    print(f\"\\n‚ö†Ô∏è Hala √ßok az topic var! Daha agresif parametreler deneniyor...\")\n",
    "\n",
    "    # √áOK K√ú√á√úK CLUSTER'LAR\n",
    "    hdbscan_aggressive = HDBSCAN(\n",
    "        min_cluster_size=max(3, min_cluster_size // 2),\n",
    "        min_samples=2,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='leaf',  # eom'dan daha agresif\n",
    "        prediction_data=True\n",
    "    )\n",
    "\n",
    "    # DAHA AZ BOYUT ƒ∞NDƒ∞RGEME\n",
    "    umap_aggressive = UMAP(\n",
    "        n_neighbors=8,\n",
    "        n_components=15,  # Daha az indirgeme\n",
    "        min_dist=0.2,\n",
    "        metric='cosine',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # YENƒ∞DEN DENE\n",
    "    topic_model_v2 = BERTopic(\n",
    "        language=\"english\",\n",
    "        umap_model=umap_aggressive,\n",
    "        hdbscan_model=hdbscan_aggressive,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        calculate_probabilities=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    print(\"üîÑ ƒ∞kinci deneme ba≈ülƒ±yor...\")\n",
    "    topics_v2, probs_v2 = topic_model_v2.fit_transform(comments)\n",
    "\n",
    "    n_topics_v2 = len(set(topics_v2))\n",
    "    print(f\"Yeni topic sayƒ±sƒ±: {n_topics_v2}\")\n",
    "\n",
    "    if n_topics_v2 > n_topics:\n",
    "        print(\"‚úÖ ƒ∞kinci deneme daha iyi!\")\n",
    "        topics = topics_v2\n",
    "        probs = probs_v2\n",
    "        topic_model = topic_model_v2\n",
    "        n_topics = n_topics_v2\n",
    "\n",
    "# 11. Topic detaylarƒ±\n",
    "df_cleaned[\"topic\"] = topics\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# 12. Anahtar kelimeler - daha anlamlƒ± mƒ±?\n",
    "print(f\"\\nüîë Topic anahtar kelimeleri:\")\n",
    "for topic_id in range(min(10, n_topics)):\n",
    "    if topic_id != -1:\n",
    "        topic_words = topic_model.get_topic(topic_id)\n",
    "        if topic_words:\n",
    "            words_scores = [(word, f\"{score:.3f}\") for word, score in topic_words[:8]]\n",
    "            words_only = [word for word, score in topic_words[:8]]\n",
    "            print(f\"Topic {topic_id}: {', '.join(words_only)}\")\n",
    "\n",
    "# 13. √á√ñZ√úM 2: Elle topic sayƒ±sƒ± belirleme\n",
    "print(f\"\\nüéØ MANUEL TOPIC SAYISI BELƒ∞RLEME:\")\n",
    "print(f\"Eƒüer sonu√ß hala k√∂t√ºyse, istediƒüiniz topic sayƒ±sƒ±nƒ± manuel belirleyin:\")\n",
    "print(f\"\")\n",
    "print(f\"# √ñrnek: 15 topic'e indirgemek i√ßin\")\n",
    "print(f\"topic_model.reduce_topics(comments, nr_topics=15)\")\n",
    "print(f\"topics_reduced = topic_model.topics_\")\n",
    "print(f\"df_cleaned['topic'] = topics_reduced\")\n",
    "\n",
    "# 14. Hƒ±zlƒ± kalite kontrol√º\n",
    "quality_score = 0\n",
    "if n_topics >= 8:\n",
    "    quality_score += 1\n",
    "if outlier_ratio < 40:\n",
    "    quality_score += 1\n",
    "\n",
    "# Topic kelimeleri anlamlƒ± mƒ± kontrol et\n",
    "meaningful_topics = 0\n",
    "for topic_id in range(min(5, n_topics)):\n",
    "    if topic_id != -1:\n",
    "        topic_words = topic_model.get_topic(topic_id)\n",
    "        if topic_words:\n",
    "            # ƒ∞lk 3 kelimeye bak, √ßok genel deƒüilse anlamlƒ± say\n",
    "            top_words = [word for word, score in topic_words[:3]]\n",
    "            generic_words = ['turkey', 'thanks', 'thank', 'good', 'great', 'nice']\n",
    "            if not any(word in generic_words for word in top_words):\n",
    "                meaningful_topics += 1\n",
    "\n",
    "if meaningful_topics >= 3:\n",
    "    quality_score += 1\n",
    "\n",
    "print(f\"\\nüìà KALƒ∞TE SKORU: {quality_score}/3\")\n",
    "if quality_score >= 2:\n",
    "    print(\"‚úÖ Sonu√ß kullanƒ±labilir!\")\n",
    "else:\n",
    "    print(\"‚ùå Hala problemli. Elle ayarlama gerekiyor.\")\n",
    "    print(\"\\nüîß SONRAKƒ∞ ADIMLAR:\")\n",
    "    print(\"1. Veri boyutunuzu kontrol edin\")\n",
    "    print(\"2. Yorumlarƒ± okuyup hangi konularƒ± beklediƒüinizi belirleyin\")\n",
    "    print(\"3. min_cluster_size'ƒ± daha da k√º√ß√ºlt√ºn (3-5 arasƒ±)\")\n",
    "    print(\"4. Belirli topic sayƒ±sƒ± i√ßin reduce_topics() kullanƒ±n\")\n",
    "\n",
    "print(f\"\\n‚ú® Final topic sayƒ±sƒ±: {len(set(topics))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667
    },
    "id": "24R-1_RhIHOq",
    "outputId": "18e9bc11-4e7f-45c6-bf6a-194a86c73e0d"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 767
    },
    "id": "YWVVd93oIhlV",
    "outputId": "9028ae8c-02a3-48c2-9e62-c5cf1ece3348"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(top_n_topics=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ic3xJmWIKOc9"
   },
   "source": [
    "### **Sentiment & Topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "-yFhhTSzJCGQ",
    "outputId": "95789fc6-e651-4a8d-e7e1-65deb5b9d960"
   },
   "outputs": [],
   "source": [
    "# Topic-sentiment relationship table\n",
    "topic_sentiment = df_cleaned.groupby([\"topic\", \"sentiment\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# If you want to normalize (percentage distribution within each topic):\n",
    "topic_sentiment_percent = topic_sentiment.div(topic_sentiment.sum(axis=1), axis=0)\n",
    "\n",
    "# Visualize (bar chart)\n",
    "topic_sentiment_percent.plot(kind='bar', stacked=True, figsize=(15,6))\n",
    "plt.title(\"Sentiment Distribution by Topics\")\n",
    "plt.xlabel(\"Topic ID\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.legend(title='Sentiment')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0qx8C-CJWUm",
    "outputId": "ffb93c14-51c4-41ca-ac66-46a100e2b9f5"
   },
   "outputs": [],
   "source": [
    "# Let's get the top 3 topics with the highest percentage for each sentiment\n",
    "top_topics_per_sentiment = {}\n",
    "\n",
    "for sentiment in topic_sentiment_percent.columns:\n",
    "    top_topics = topic_sentiment_percent[sentiment].sort_values(ascending=False).head(3)\n",
    "    top_topics_per_sentiment[sentiment] = top_topics\n",
    "\n",
    "# Print the results\n",
    "for sentiment, topics in top_topics_per_sentiment.items():\n",
    "    print(f\"\\nüîπ Top 3 Dominant Topics for {sentiment.upper()} Sentiment:\")\n",
    "    print(topics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LBarnA4Rfi9"
   },
   "source": [
    "## **5 keywords for the most positive and most negative topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997
    },
    "id": "9-Nh_EtjtdA8",
    "outputId": "373348ea-9a4b-4129-a029-342acfcb88a4"
   },
   "outputs": [],
   "source": [
    "# Her sentiment i√ßin farklƒ± renk seti belirleyebiliriz\n",
    "colors = {\n",
    "    'NEGATIVE': '#e74c3c',\n",
    "    'POSITIVE': '#2ecc71',\n",
    "    'NEUTRAL': '#3498db'\n",
    "}\n",
    "\n",
    "# Grafik √ßizimi\n",
    "for sentiment, topics in top_topics_per_sentiment.items():\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    topic_labels = [f\"Topic {topic}\" for topic in topics.index]\n",
    "    scores = topics.values\n",
    "\n",
    "    descriptions = []\n",
    "    for topic in topics.index:\n",
    "        try:\n",
    "            top_words = topic_model.get_topic(topic)\n",
    "            if top_words:\n",
    "                descriptions.append(' | '.join([word for word, _ in top_words[:5]]))\n",
    "            else:\n",
    "                descriptions.append(\"No topic found\")\n",
    "        except Exception as e:\n",
    "            descriptions.append(\"Error\")\n",
    "\n",
    "    plt.barh(topic_labels, scores, color=colors.get(sentiment.upper(), 'gray'))\n",
    "    plt.xlabel(\"Sentiment Oranƒ±\")\n",
    "    plt.title(f\"{sentiment.upper()} Sentiment ‚Äì En Baskƒ±n 5 Topic\")\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    for i, (score, desc) in enumerate(zip(scores, descriptions)):\n",
    "        plt.text(score + 0.01, i, desc, va='center', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07127d9df12c49859bd405669d173959": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ca71567bf67f40a89f2e3e3502c47225",
       "IPY_MODEL_b110ff373a9f4ef7b53c92b46234c587",
       "IPY_MODEL_cbb7369fbd2b40178e4429044c45bb6f"
      ],
      "layout": "IPY_MODEL_6fccd1f6dd784bd79221b4d20db74924"
     }
    },
    "30734bce91574140ab2e6545f1321012": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31eb8b3700b844e59fc08187f6db81e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f1b085f2f9d4c3bba056392bbfad65a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fccd1f6dd784bd79221b4d20db74924": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae1088fda5294b888973072cb2d283a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b110ff373a9f4ef7b53c92b46234c587": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30734bce91574140ab2e6545f1321012",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ae1088fda5294b888973072cb2d283a7",
      "value": 28
     }
    },
    "ca71567bf67f40a89f2e3e3502c47225": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e206a6ff2113435ebcb7c18f1b8050f3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d02daa13047e43bbb99934d14af4ea44",
      "value": "Batches:‚Äá100%"
     }
    },
    "cbb7369fbd2b40178e4429044c45bb6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31eb8b3700b844e59fc08187f6db81e8",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5f1b085f2f9d4c3bba056392bbfad65a",
      "value": "‚Äá28/28‚Äá[00:16&lt;00:00,‚Äá‚Äá4.89it/s]"
     }
    },
    "d02daa13047e43bbb99934d14af4ea44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e206a6ff2113435ebcb7c18f1b8050f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
